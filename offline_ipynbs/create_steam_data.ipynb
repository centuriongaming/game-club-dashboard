{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6WRVlA21pRN"
      },
      "outputs": [],
      "source": [
        "pip install pandas requests tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas requests tqdm beautifulsoup4 lxml"
      ],
      "metadata": {
        "id": "xFC2nSMR8_M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_FILENAME = 'games_list.csv'\n",
        "OUTPUT_FILENAME = 'game_features.csv'\n",
        "API_CALL_DELAY_SECONDS = 1.5  # Delay between requests to be respectful to Steam's servers.\n",
        "\n",
        "def get_game_details(app_id: int) -> dict | None:\n",
        "    \"\"\"\n",
        "    Fetches details from the Steam API and scrapes the store page for user tags.\n",
        "    \"\"\"\n",
        "    # --- Part 1: Get structured data from the official API ---\n",
        "    api_url = f\"https://store.steampowered.com/api/appdetails?appids={app_id}&cc=us\"\n",
        "    game_details = {}\n",
        "\n",
        "    try:\n",
        "        api_response = requests.get(api_url, timeout=15)\n",
        "        api_response.raise_for_status()\n",
        "        api_data = api_response.json()\n",
        "        app_id_str = str(app_id)\n",
        "\n",
        "        if api_data and app_id_str in api_data and api_data[app_id_str].get('success'):\n",
        "            game_data = api_data[app_id_str]['data']\n",
        "\n",
        "            genres_list = game_data.get('genres', [])\n",
        "            developer_genres = ', '.join([g['description'] for g in genres_list]) if genres_list else 'N/A'\n",
        "\n",
        "            if game_data.get('is_free', False):\n",
        "                price_usd = 0.0\n",
        "            elif 'price_overview' in game_data:\n",
        "                price_usd = game_data['price_overview'].get('final', 0) / 100.0\n",
        "            else:\n",
        "                price_usd = 'N/A'\n",
        "\n",
        "            # Store the data retrieved from the API\n",
        "            game_details = {\n",
        "                'id': app_id,\n",
        "                'name': game_data.get('name', 'N/A'),\n",
        "                'developer_genres': developer_genres,\n",
        "                'price_usd': price_usd,\n",
        "                'metacritic_score': game_data.get('metacritic', {}).get('score', 'N/A'),\n",
        "                'release_date': game_data.get('release_date', {}).get('date', 'N/A'),\n",
        "                'developers': ', '.join(game_data.get('developers', ['N/A'])),\n",
        "                'publishers': ', '.join(game_data.get('publishers', ['N/A'])),\n",
        "            }\n",
        "        else:\n",
        "            tqdm.write(f\"Warning: API data not found for App ID {app_id}. Skipping.\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        tqdm.write(f\"Error fetching API data for App ID {app_id}: {e}\")\n",
        "        return None  # If the API call fails, we can't proceed with this item.\n",
        "\n",
        "    # --- Part 2: Scrape the public store page to get user tags ---\n",
        "    store_url = f\"https://store.steampowered.com/app/{app_id}/\"\n",
        "    user_tags = '[]'  # Default to an empty JSON array string\n",
        "\n",
        "    try:\n",
        "        # This cookie is crucial to bypass age verification pages for mature games.\n",
        "        cookies = {'birthtime': '568022401', 'wants_mature_content': '1'}\n",
        "        store_response = requests.get(store_url, cookies=cookies, timeout=15)\n",
        "        store_response.raise_for_status()\n",
        "\n",
        "        # Parse the page HTML\n",
        "        soup = BeautifulSoup(store_response.text, 'lxml')\n",
        "\n",
        "        # Find all hyperlink tags with the CSS class 'app_tag'.\n",
        "        tag_elements = soup.find_all('a', class_='app_tag')\n",
        "\n",
        "        if tag_elements:\n",
        "            # Extract the clean text from each tag and store it in a list.\n",
        "            tag_list = [tag.get_text(strip=True) for tag in tag_elements]\n",
        "            user_tags = json.dumps(tag_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        # This can happen if a page is not found, has a different layout, or on a network error.\n",
        "        tqdm.write(f\"Warning: Could not scrape tags for App ID {app_id}. Reason: {e}\")\n",
        "        # We will still return the data from the API, but the tags will be empty.\n",
        "\n",
        "    # Add the scraped tags to our dictionary and return the complete record.\n",
        "    game_details['user_tags'] = user_tags\n",
        "    return game_details\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to read App IDs, fetch data, and write to a new CSV.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(INPUT_FILENAME):\n",
        "        print(f\"Error: Input file '{INPUT_FILENAME}' not found.\")\n",
        "        print(f\"Creating a sample '{INPUT_FILENAME}' with some App IDs...\")\n",
        "        sample_df = pd.DataFrame({\n",
        "            'id': [620, 730, 578080], # Portal 2, CS:GO, PUBG\n",
        "            'some_other_column': ['will be ignored', 'also ignored', 'ignored']\n",
        "        })\n",
        "        sample_df.to_csv(INPUT_FILENAME, index=False)\n",
        "        print(\"Please populate this file with your desired Steam App IDs and run the script again.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        input_df = pd.read_csv(INPUT_FILENAME)\n",
        "        if 'id' not in input_df.columns:\n",
        "            print(f\"Error: Input CSV '{INPUT_FILENAME}' must contain an 'id' column.\")\n",
        "            return\n",
        "        app_ids = input_df['id'].dropna().unique().astype(int).tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or parsing '{INPUT_FILENAME}': {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(app_ids)} unique App IDs to process.\")\n",
        "\n",
        "    all_game_data = []\n",
        "    for app_id in tqdm(app_ids, desc=\"Fetching game data from Steam\"):\n",
        "        details = get_game_details(app_id)\n",
        "        if details:\n",
        "            all_game_data.append(details)\n",
        "        time.sleep(API_CALL_DELAY_SECONDS)\n",
        "\n",
        "    if not all_game_data:\n",
        "        print(\"No data was successfully fetched. The output file will not be created.\")\n",
        "        return\n",
        "\n",
        "    output_df = pd.DataFrame(all_game_data)\n",
        "\n",
        "    final_columns = [\n",
        "        'id', 'name', 'user_tags', 'developer_genres', 'price_usd',\n",
        "        'metacritic_score', 'release_date', 'developers', 'publishers'\n",
        "    ]\n",
        "    output_df = output_df[final_columns]\n",
        "\n",
        "    output_df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n✅ Success! Processed {len(output_df)} games.\")\n",
        "    print(f\"Data saved to '{OUTPUT_FILENAME}'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "jL0I9-qi6R4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "import argparse\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Configuration is now handled by argparse ---\n",
        "MIN_RATINGS_PER_CRITIC = 10\n",
        "\n",
        "# (The preprocess_data and build_feature_pipeline functions remain exactly the same as before)\n",
        "# ... copy them here ...\n",
        "def preprocess_data(games_df, ratings_df):\n",
        "    \"\"\"Merges, cleans, and prepares the data for modeling.\"\"\"\n",
        "    if 'appid' in games_df.columns:\n",
        "        games_df = games_df.rename(columns={'appid': 'id'})\n",
        "    merged_df = pd.merge(ratings_df, games_df, left_on='game_id', right_on='id', how='left')\n",
        "    merged_df['will_skip'] = merged_df['score'].isnull() | (merged_df['score'] == 'skipped')\n",
        "    merged_df['score_numeric'] = pd.to_numeric(merged_df['score'], errors='coerce')\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        merged_df[col] = merged_df[col].fillna('[]' if 'tags' in col else '')\n",
        "    def parse_tags(tags_str):\n",
        "        try:\n",
        "            tags_list = json.loads(tags_str.replace(\"'\", \"\\\"\"))\n",
        "            return ' '.join(tags_list)\n",
        "        except: return ''\n",
        "    if 'user_tags' in merged_df.columns:\n",
        "         merged_df['user_tags'] = merged_df['user_tags'].apply(parse_tags)\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "         merged_df[col] = merged_df[col].str.replace(',', ' ')\n",
        "    merged_df['metacritic_score'] = pd.to_numeric(merged_df['metacritic_score'], errors='coerce')\n",
        "    merged_df['price_usd'] = pd.to_numeric(merged_df['price_usd'], errors='coerce')\n",
        "    merged_df['release_year'] = pd.to_datetime(merged_df['release_date'], errors='coerce').dt.year\n",
        "    return merged_df\n",
        "\n",
        "def build_feature_pipeline():\n",
        "    \"\"\"Builds a scikit-learn pipeline to transform features.\"\"\"\n",
        "    numeric_features = ['metacritic_score', 'price_usd', 'release_year']\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "    text_features = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    text_transformer = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('tags', text_transformer, 'user_tags'),\n",
        "            ('genres', TfidfVectorizer(max_features=50), 'developer_genres'),\n",
        "            ('devs', TfidfVectorizer(max_features=50), 'developers'),\n",
        "            ('pubs', TfidfVectorizer(max_features=50), 'publishers')\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"Main function to run the analysis.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    games_path = os.path.join(args.data_dir, 'games_details.csv')\n",
        "    ratings_path = os.path.join(args.data_dir, 'ratings.csv')\n",
        "    try:\n",
        "        games_df = pd.read_csv(games_path)\n",
        "        ratings_df = pd.read_csv(ratings_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Make sure your CSV files are in the '{args.data_dir}' directory.\")\n",
        "        return\n",
        "\n",
        "    # Ensure output directories exist\n",
        "    os.makedirs(args.models_dir, exist_ok=True)\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data = preprocess_data(games_df, ratings_df)\n",
        "\n",
        "    # Prepare all_games_features DataFrame for prediction\n",
        "    all_games_features = games_df.drop_duplicates(subset=['id']).copy()\n",
        "    # (Apply same preprocessing as in preprocess_data)\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        all_games_features[col] = all_games_features[col].fillna('[]' if 'tags' in col else '')\n",
        "    if 'user_tags' in all_games_features.columns:\n",
        "        all_games_features['user_tags'] = all_games_features['user_tags'].apply(lambda x: ' '.join(json.loads(x.replace(\"'\", \"\\\"\"))) if x.startswith('[') else '')\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "         all_games_features[col] = all_games_features[col].str.replace(',', ' ')\n",
        "    all_games_features['metacritic_score'] = pd.to_numeric(all_games_features['metacritic_score'], errors='coerce')\n",
        "    all_games_features['price_usd'] = pd.to_numeric(all_games_features['price_usd'], errors='coerce')\n",
        "    all_games_features['release_year'] = pd.to_datetime(all_games_features['release_date'], errors='coerce').dt.year\n",
        "\n",
        "\n",
        "    # --- Model Training/Loading Loop ---\n",
        "    critics = data['critic_id'].unique()\n",
        "    all_predictions = []\n",
        "    all_importances = []\n",
        "\n",
        "    print(f\"Found {len(critics)} critics. Starting model processing loop...\")\n",
        "    for critic_id in critics:\n",
        "        critic_data = data[data['critic_id'] == critic_id]\n",
        "\n",
        "        if len(critic_data) < MIN_RATINGS_PER_CRITIC:\n",
        "            print(f\"Skipping critic {critic_id}: not enough ratings.\")\n",
        "            continue\n",
        "\n",
        "        clf_path = os.path.join(args.models_dir, f'{critic_id}_classifier.joblib')\n",
        "        reg_path = os.path.join(args.models_dir, f'{critic_id}_regressor.joblib')\n",
        "\n",
        "        # Check if models exist and we are not forcing a retrain\n",
        "        if os.path.exists(clf_path) and os.path.exists(reg_path) and not args.force_retrain:\n",
        "            print(f\"Loading existing models for critic: {critic_id}\")\n",
        "            clf_pipeline = joblib.load(clf_path)\n",
        "            reg_pipeline = joblib.load(reg_path)\n",
        "        else:\n",
        "            print(f\"--- Training new models for critic: {critic_id} ---\")\n",
        "            feature_pipeline = build_feature_pipeline()\n",
        "\n",
        "            # Train Classifier\n",
        "            X_class, y_class = critic_data, critic_data['will_skip']\n",
        "            clf_pipeline = Pipeline(steps=[('preprocessor', feature_pipeline), ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))])\n",
        "            clf_pipeline.fit(X_class, y_class)\n",
        "            joblib.dump(clf_pipeline, clf_path)\n",
        "            print(f\"  -> Saved classifier to {clf_path}\")\n",
        "\n",
        "            # Train Regressor\n",
        "            rated_games = critic_data.dropna(subset=['score_numeric'])\n",
        "            if len(rated_games) < MIN_RATINGS_PER_CRITIC / 2:\n",
        "                print(f\"  -> Not enough rated games to train regressor. Skipping.\")\n",
        "                continue\n",
        "            X_reg, y_reg = rated_games, rated_games['score_numeric']\n",
        "            reg_pipeline = Pipeline(steps=[('preprocessor', feature_pipeline), ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])\n",
        "            reg_pipeline.fit(X_reg, y_reg)\n",
        "            joblib.dump(reg_pipeline, reg_path)\n",
        "            print(f\"  -> Saved regressor to {reg_path}\")\n",
        "\n",
        "        # --- Generate Predictions for ALL games ---\n",
        "        skip_probs = clf_pipeline.predict_proba(all_games_features)[:, 1]\n",
        "        pred_scores = reg_pipeline.predict(all_games_features)\n",
        "\n",
        "        predictions_df = all_games_features[['id', 'name']].copy()\n",
        "        predictions_df['critic_id'] = critic_id\n",
        "        predictions_df['predicted_skip_probability'] = skip_probs\n",
        "        predictions_df['predicted_score'] = pred_scores\n",
        "        all_predictions.append(predictions_df)\n",
        "\n",
        "    # --- Save Results ---\n",
        "    if all_predictions:\n",
        "        pd.concat(all_predictions, ignore_index=True).to_csv(\n",
        "            os.path.join(args.results_dir, 'critic_predictions.csv'), index=False\n",
        "        )\n",
        "        print(f\"\\n✅ Predictions saved to '{args.results_dir}/critic_predictions.csv'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"Train critic models and generate game predictions.\")\n",
        "    parser.add_argument('--data_dir', type=str, default='data', help='Directory containing input CSV files.')\n",
        "    parser.add_argument('--models_dir', type=str, default='saved_models', help='Directory to save/load trained models.')\n",
        "    parser.add_argument('--results_dir', type=str, default='results', help='Directory to save output predictions.')\n",
        "    parser.add_argument('--force-retrain', action='store_true', help='Force retraining of all models, even if they exist.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "Ra5fx9io6oc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTGuO8GFDOhp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}