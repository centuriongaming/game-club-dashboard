{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "import ast\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Configuration for Local Environment ---\n",
        "DATA_DIR = 'data'\n",
        "MODELS_DIR = 'saved_models'\n",
        "RESULTS_DIR = 'results'\n",
        "\n",
        "# Set to True if you want to rebuild all models from scratch\n",
        "FORCE_RETRAIN = False\n",
        "\n",
        "# Minimum ratings required to build a model for a critic\n",
        "MIN_RATINGS_PER_CRITIC = 5"
      ],
      "metadata": {
        "id": "xRRfBF_eEwqH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tags(data_str):\n",
        "    \"\"\"\n",
        "    Safely parses a string that could be a single or double-encoded list of tags.\n",
        "    \"\"\"\n",
        "    if not isinstance(data_str, str):\n",
        "        return ''\n",
        "    try:\n",
        "        # First, try to evaluate the string as a Python literal.\n",
        "        parsed_data = ast.literal_eval(data_str)\n",
        "\n",
        "        # If the result of the first parse is *still* a string, it's likely\n",
        "        # double-encoded, so we parse it again using json.loads.\n",
        "        if isinstance(parsed_data, str):\n",
        "            final_list = json.loads(parsed_data)\n",
        "        else:\n",
        "            final_list = parsed_data\n",
        "\n",
        "        # Final check to ensure we have a list before joining.\n",
        "        if isinstance(final_list, list):\n",
        "            return ' '.join(final_list)\n",
        "        else:\n",
        "            return ''\n",
        "    except (ValueError, SyntaxError, json.JSONDecodeError, TypeError):\n",
        "        # If any parsing step fails, return an empty string.\n",
        "        return ''\n",
        "\n",
        "def preprocess_data(games_df, ratings_df):\n",
        "    \"\"\"Merges, cleans, and prepares the data for modeling.\"\"\"\n",
        "    ratings_df = ratings_df.rename(columns={'id': 'rating_id'})\n",
        "    if 'appid' in games_df.columns:\n",
        "        games_df = games_df.drop(columns=['appid'])\n",
        "\n",
        "    merged_df = pd.merge(ratings_df, games_df, left_on='game_id', right_on='id', how='left')\n",
        "\n",
        "    merged_df['will_skip'] = merged_df['score'].isnull() | (merged_df['score'] == 'skipped')\n",
        "    merged_df['score_numeric'] = pd.to_numeric(merged_df['score'], errors='coerce')\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        merged_df[col] = merged_df[col].fillna('')\n",
        "\n",
        "    if 'user_tags' in merged_df.columns:\n",
        "         merged_df['user_tags'] = merged_df['user_tags'].apply(parse_tags) # Use new parser\n",
        "\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "        if col in merged_df:\n",
        "             merged_df[col] = merged_df[col].str.replace(',', ' ')\n",
        "\n",
        "    merged_df['metacritic_score'] = pd.to_numeric(merged_df['metacritic_score'], errors='coerce')\n",
        "    merged_df['price_usd'] = pd.to_numeric(merged_df['price_usd'], errors='coerce')\n",
        "    merged_df['release_year'] = pd.to_datetime(merged_df['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def build_feature_pipeline():\n",
        "    \"\"\"Builds a scikit-learn pipeline to transform raw data into model-ready features.\"\"\"\n",
        "    numeric_features = ['metacritic_score', 'price_usd', 'release_year']\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    text_transformer = TfidfVectorizer(stop_words='english', max_features=100, min_df=1)\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('tags', text_transformer, 'user_tags'),\n",
        "            ('genres', TfidfVectorizer(max_features=50, min_df=1), 'developer_genres'),\n",
        "            ('devs', TfidfVectorizer(max_features=50, min_df=1), 'developers'),\n",
        "            ('pubs', TfidfVectorizer(max_features=50, min_df=1), 'publishers')\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor"
      ],
      "metadata": {
        "id": "79OrBMu1GhRy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_process(data_dir, models_dir, results_dir, force_retrain):\n",
        "    \"\"\"Main function to run the entire analysis pipeline.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    games_path = os.path.join(data_dir, 'games_details.csv')\n",
        "    ratings_path = os.path.join(data_dir, 'ratings.csv')\n",
        "    try:\n",
        "        games_df = pd.read_csv(games_path)\n",
        "        ratings_df = pd.read_csv(ratings_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Make sure your CSV files are in the '{data_dir}' directory.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data = preprocess_data(games_df.copy(), ratings_df.copy())\n",
        "\n",
        "    all_games_features = games_df.drop_duplicates(subset=['id']).copy()\n",
        "    if 'appid' in all_games_features.columns:\n",
        "        all_games_features = all_games_features.drop(columns=['appid'])\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        all_games_features[col] = all_games_features[col].fillna('')\n",
        "    if 'user_tags' in all_games_features.columns:\n",
        "        all_games_features['user_tags'] = all_games_features['user_tags'].apply(parse_tags)\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "        if col in all_games_features:\n",
        "             all_games_features[col] = all_games_features[col].str.replace(',', ' ')\n",
        "\n",
        "    all_games_features['metacritic_score'] = pd.to_numeric(all_games_features['metacritic_score'], errors='coerce')\n",
        "    all_games_features['price_usd'] = pd.to_numeric(all_games_features['price_usd'], errors='coerce')\n",
        "    all_games_features['release_year'] = pd.to_datetime(all_games_features['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "    print(\"Fitting the feature preprocessor on all games...\")\n",
        "    preprocessor = build_feature_pipeline()\n",
        "    preprocessor.fit(all_games_features)\n",
        "    print(\"Preprocessor fitted.\")\n",
        "\n",
        "    # --- NEW: Get feature names from the fitted preprocessor ---\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not get feature names from the preprocessor. Error: {e}\")\n",
        "        feature_names = None\n",
        "\n",
        "    critics = data['critic_id'].unique()\n",
        "    all_predictions = []\n",
        "    all_importances = [] # --- NEW: List to store importance results ---\n",
        "\n",
        "    print(f\"\\nFound {len(critics)} critics. Starting model processing loop...\")\n",
        "    for critic_id in critics:\n",
        "        critic_data = data[data['critic_id'] == critic_id]\n",
        "\n",
        "        if len(critic_data) < MIN_RATINGS_PER_CRITIC:\n",
        "            print(f\"Skipping critic {critic_id}: not enough ratings.\")\n",
        "            continue\n",
        "\n",
        "        clf_path = os.path.join(models_dir, f'{critic_id}_classifier.joblib')\n",
        "        reg_path = os.path.join(models_dir, f'{critic_id}_regressor.joblib')\n",
        "\n",
        "        print(f\"--- Training models for critic: {critic_id} ---\")\n",
        "\n",
        "        X_class_transformed = preprocessor.transform(critic_data)\n",
        "        y_class = critic_data['will_skip']\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        clf.fit(X_class_transformed, y_class)\n",
        "        joblib.dump(clf, clf_path)\n",
        "\n",
        "        rated_games = critic_data.dropna(subset=['score_numeric'])\n",
        "        reg = None\n",
        "        if len(rated_games) >= MIN_RATINGS_PER_CRITIC / 2:\n",
        "            X_reg_transformed = preprocessor.transform(rated_games)\n",
        "            y_reg = rated_games['score_numeric']\n",
        "            reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            reg.fit(X_reg_transformed, y_reg)\n",
        "            joblib.dump(reg, reg_path)\n",
        "\n",
        "        # --- NEW: Extract and store top 5 feature importances ---\n",
        "        if feature_names is not None:\n",
        "            # Get importances for the skip model (classifier)\n",
        "            skip_importances = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': clf.feature_importances_\n",
        "            }).sort_values(by='importance', ascending=False).head(5)\n",
        "            skip_importances['critic_id'] = critic_id\n",
        "            skip_importances['model_type'] = 'skip_prediction'\n",
        "            all_importances.append(skip_importances)\n",
        "\n",
        "            # Get importances for the score model (regressor), if it was trained\n",
        "            if reg:\n",
        "                score_importances = pd.DataFrame({\n",
        "                    'feature': feature_names,\n",
        "                    'importance': reg.feature_importances_\n",
        "                }).sort_values(by='importance', ascending=False).head(5)\n",
        "                score_importances['critic_id'] = critic_id\n",
        "                score_importances['model_type'] = 'score_prediction'\n",
        "                all_importances.append(score_importances)\n",
        "\n",
        "        # --- Generate Predictions for ALL games ---\n",
        "        X_all_games_transformed = preprocessor.transform(all_games_features)\n",
        "        skip_probs = clf.predict_proba(X_all_games_transformed)[:, 1]\n",
        "        pred_scores = reg.predict(X_all_games_transformed) if reg else np.nan\n",
        "\n",
        "        predictions_df = all_games_features[['id', 'name']].copy()\n",
        "        predictions_df['critic_id'] = critic_id\n",
        "        predictions_df['predicted_skip_probability'] = skip_probs\n",
        "        predictions_df['predicted_score'] = pred_scores\n",
        "        all_predictions.append(predictions_df)\n",
        "\n",
        "    # --- Save Prediction Results ---\n",
        "    if all_predictions:\n",
        "        final_pred_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        pred_output_path = os.path.join(results_dir, 'critic_predictions.csv')\n",
        "        final_pred_df.to_csv(pred_output_path, index=False)\n",
        "        print(f\"\\n✅ Predictions saved to '{pred_output_path}'\")\n",
        "    else:\n",
        "        print(\"\\nNo predictions were generated.\")\n",
        "\n",
        "    # --- NEW: Save Feature Importances to a CSV ---\n",
        "    if all_importances:\n",
        "        final_imp_df = pd.concat(all_importances, ignore_index=True)\n",
        "        imp_output_path = os.path.join(results_dir, 'critic_feature_importances.csv')\n",
        "        final_imp_df.to_csv(imp_output_path, index=False)\n",
        "        print(f\"✅ Feature importances saved to '{imp_output_path}'\")"
      ],
      "metadata": {
        "id": "uoZ2gpTxGwxs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_process(\n",
        "    data_dir=DATA_DIR,\n",
        "    models_dir=MODELS_DIR,\n",
        "    results_dir=RESULTS_DIR,\n",
        "    force_retrain=FORCE_RETRAIN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdMPMdxHGz9J",
        "outputId": "9208866b-41e2-470a-9bd4-0b6d1d0020b0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Preprocessing data...\n",
            "Fitting the feature preprocessor on all games...\n",
            "Preprocessor fitted.\n",
            "\n",
            "Found 10 critics. Starting model processing loop...\n",
            "--- Training models for critic: 1 ---\n",
            "--- Training models for critic: 2 ---\n",
            "--- Training models for critic: 3 ---\n",
            "--- Training models for critic: 4 ---\n",
            "--- Training models for critic: 5 ---\n",
            "--- Training models for critic: 6 ---\n",
            "--- Training models for critic: 7 ---\n",
            "--- Training models for critic: 8 ---\n",
            "--- Training models for critic: 10 ---\n",
            "--- Training models for critic: 9 ---\n",
            "\n",
            "✅ Predictions saved to 'results/critic_predictions.csv'\n",
            "✅ Feature importances saved to 'results/critic_feature_importances.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the saved_models directory\n",
        "!zip -r /content/saved_models.zip /content/saved_models\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('/content/saved_models.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "P-_0kvrDG1KP",
        "outputId": "38342e0d-fa79-40bd-d1d6-c7463abeeb86"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/saved_models/ (stored 0%)\n",
            "  adding: content/saved_models/7_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/3_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/5_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/10_regressor.joblib (deflated 92%)\n",
            "  adding: content/saved_models/4_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/2_regressor.joblib (deflated 89%)\n",
            "  adding: content/saved_models/2_classifier.joblib (deflated 89%)\n",
            "  adding: content/saved_models/1_regressor.joblib (deflated 90%)\n",
            "  adding: content/saved_models/8_regressor.joblib (deflated 89%)\n",
            "  adding: content/saved_models/5_regressor.joblib (deflated 91%)\n",
            "  adding: content/saved_models/6_classifier.joblib (deflated 93%)\n",
            "  adding: content/saved_models/6_regressor.joblib (deflated 87%)\n",
            "  adding: content/saved_models/9_classifier.joblib (deflated 93%)\n",
            "  adding: content/saved_models/8_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/7_regressor.joblib (deflated 92%)\n",
            "  adding: content/saved_models/4_regressor.joblib (deflated 88%)\n",
            "  adding: content/saved_models/10_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/1_classifier.joblib (deflated 87%)\n",
            "  adding: content/saved_models/3_regressor.joblib (deflated 90%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a617af27-ad39-4d0b-926d-a63f0d66e4dc\", \"saved_models.zip\", 223399)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}