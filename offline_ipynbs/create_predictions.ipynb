{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "import ast\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Configuration for Local Environment ---\n",
        "DATA_DIR = 'data'\n",
        "MODELS_DIR = 'saved_models'\n",
        "RESULTS_DIR = 'results'\n",
        "\n",
        "# Set to True if you want to rebuild all models from scratch\n",
        "FORCE_RETRAIN = False\n",
        "\n",
        "# Minimum ratings required to build a model for a critic\n",
        "MIN_RATINGS_PER_CRITIC = 5"
      ],
      "metadata": {
        "id": "xRRfBF_eEwqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_fields(text_data):\n",
        "    \"\"\"\n",
        "    Cleans comma-separated text fields by turning each item into a single token.\n",
        "    Example: '5 lives studios, Valve' becomes '5_lives_studios Valve'\n",
        "    \"\"\"\n",
        "    if not isinstance(text_data, str):\n",
        "        return ''\n",
        "\n",
        "    # Split by comma, strip whitespace, replace internal spaces with underscores\n",
        "    items = [item.strip().replace(' ', '_') for item in text_data.split(',')]\n",
        "\n",
        "    # Join the processed items back with a space for the vectorizer\n",
        "    return ' '.join(items)"
      ],
      "metadata": {
        "id": "eMUmOfUON2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json\n",
        "\n",
        "def parse_tags(data_str):\n",
        "    \"\"\"\n",
        "    Safely parses a string that could be a single or double-encoded list of tags.\n",
        "    Replaces spaces and hyphens in multi-word tags with underscores.\n",
        "    \"\"\"\n",
        "    if not isinstance(data_str, str):\n",
        "        return ''\n",
        "    try:\n",
        "        # First, try to evaluate the string as a Python literal.\n",
        "        parsed_data = ast.literal_eval(data_str)\n",
        "\n",
        "        # If the result of the first parse is *still* a string, it's likely\n",
        "        # double-encoded, so we parse it again using json.loads.\n",
        "        if isinstance(parsed_data, str):\n",
        "            final_list = json.loads(parsed_data)\n",
        "        else:\n",
        "            final_list = parsed_data\n",
        "\n",
        "        # Final check to ensure we have a list before joining.\n",
        "        if isinstance(final_list, list):\n",
        "            # Replace spaces and hyphens with underscores in each tag\n",
        "            processed_tags = [tag.replace(' ', '_').replace('-', '_') for tag in final_list]\n",
        "            return ' '.join(processed_tags)\n",
        "        else:\n",
        "            return ''\n",
        "    except (ValueError, SyntaxError, json.JSONDecodeError, TypeError):\n",
        "        # If any parsing step fails, return an empty string.\n",
        "        return ''\n",
        "\n",
        "def preprocess_data(games_df, ratings_df):\n",
        "    \"\"\"Merges, cleans, and prepares the data for modeling.\"\"\"\n",
        "    ratings_df = ratings_df.rename(columns={'id': 'rating_id'})\n",
        "    if 'appid' in games_df.columns:\n",
        "        games_df = games_df.drop(columns=['appid'])\n",
        "\n",
        "    merged_df = pd.merge(ratings_df, games_df, left_on='game_id', right_on='id', how='left')\n",
        "\n",
        "    merged_df['will_skip'] = merged_df['score'].isnull() | (merged_df['score'] == 'skipped')\n",
        "    merged_df['score_numeric'] = pd.to_numeric(merged_df['score'], errors='coerce')\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        merged_df[col] = merged_df[col].fillna('')\n",
        "\n",
        "    if 'user_tags' in merged_df.columns:\n",
        "         merged_df['user_tags'] = merged_df['user_tags'].apply(parse_tags) # Use new parser\n",
        "\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "            if col in merged_df:\n",
        "                merged_df[col] = merged_df[col].apply(process_text_fields)\n",
        "\n",
        "    merged_df['metacritic_score'] = pd.to_numeric(merged_df['metacritic_score'], errors='coerce')\n",
        "    merged_df['price_usd'] = pd.to_numeric(merged_df['price_usd'], errors='coerce')\n",
        "    merged_df['release_year'] = pd.to_datetime(merged_df['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def build_feature_pipeline():\n",
        "    \"\"\"Generates a scikit-learn pipeline to transform raw data into model-ready features.\"\"\"\n",
        "    numeric_features = ['metacritic_score', 'price_usd', 'release_year']\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    # Modify TfidfVectorizer for user_tags to include all n-grams\n",
        "    text_transformer_tags = TfidfVectorizer(stop_words='english', max_features=100, min_df=1) # ngram_range can be default now\n",
        "    text_transformer_genres = TfidfVectorizer(max_features=50, min_df=1)\n",
        "    text_transformer_devs = TfidfVectorizer(max_features=50, min_df=1)\n",
        "    text_transformer_pubs = TfidfVectorizer(max_features=50, min_df=1)\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('tags', text_transformer_tags, 'user_tags'),\n",
        "            ('genres', text_transformer_genres, 'developer_genres'),\n",
        "            ('devs', text_transformer_devs, 'developers'),\n",
        "            ('pubs', text_transformer_pubs, 'publishers')\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor"
      ],
      "metadata": {
        "id": "79OrBMu1GhRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_process(data_dir, models_dir, results_dir, force_retrain):\n",
        "    \"\"\"Main function to run the entire analysis pipeline.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    games_path = os.path.join(data_dir, 'games_details.csv')\n",
        "    ratings_path = os.path.join(data_dir, 'ratings.csv')\n",
        "    try:\n",
        "        games_df = pd.read_csv(games_path)\n",
        "        ratings_df = pd.read_csv(ratings_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Make sure your CSV files are in the '{data_dir}' directory.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data = preprocess_data(games_df.copy(), ratings_df.copy())\n",
        "\n",
        "    all_games_features = games_df.drop_duplicates(subset=['id']).copy()\n",
        "    if 'appid' in all_games_features.columns:\n",
        "        all_games_features = all_games_features.drop(columns=['appid'])\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        all_games_features[col] = all_games_features[col].fillna('')\n",
        "    if 'user_tags' in all_games_features.columns:\n",
        "        all_games_features['user_tags'] = all_games_features['user_tags'].apply(parse_tags)\n",
        "\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "            if col in all_games_features:\n",
        "                all_games_features[col] = all_games_features[col].apply(process_text_fields)\n",
        "\n",
        "    all_games_features['metacritic_score'] = pd.to_numeric(all_games_features['metacritic_score'], errors='coerce')\n",
        "    all_games_features['price_usd'] = pd.to_numeric(all_games_features['price_usd'], errors='coerce')\n",
        "    all_games_features['release_year'] = pd.to_datetime(all_games_features['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "    print(\"Fitting the feature preprocessor on all games...\")\n",
        "    preprocessor = build_feature_pipeline()\n",
        "    preprocessor.fit(all_games_features)\n",
        "    joblib.dump(preprocessor, os.path.join(models_dir, 'preprocessor.joblib'))\n",
        "    print(\"Preprocessor fitted and saved.\")\n",
        "\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not get feature names from the preprocessor. Error: {e}\")\n",
        "        feature_names = None\n",
        "\n",
        "    critics = data['critic_id'].unique()\n",
        "    all_predictions = []\n",
        "    all_importances = []\n",
        "    all_best_trees = [] # <-- NEW: List to store best tree info\n",
        "\n",
        "    print(f\"\\nFound {len(critics)} critics. Starting model processing loop...\")\n",
        "    for critic_id in critics:\n",
        "        critic_data = data[data['critic_id'] == critic_id]\n",
        "\n",
        "        if len(critic_data) < MIN_RATINGS_PER_CRITIC:\n",
        "            print(f\"Skipping critic {critic_id}: not enough ratings.\")\n",
        "            continue\n",
        "\n",
        "        clf_path = os.path.join(models_dir, f'{critic_id}_classifier.joblib')\n",
        "        reg_path = os.path.join(models_dir, f'{critic_id}_regressor.joblib')\n",
        "\n",
        "        print(f\"--- Training models for critic: {critic_id} ---\")\n",
        "\n",
        "        X_class_transformed = preprocessor.transform(critic_data)\n",
        "        y_class = critic_data['will_skip']\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        clf.fit(X_class_transformed, y_class)\n",
        "        joblib.dump(clf, clf_path)\n",
        "\n",
        "        rated_games = critic_data.dropna(subset=['score_numeric'])\n",
        "        reg = None\n",
        "        if len(rated_games) >= MIN_RATINGS_PER_CRITIC / 2:\n",
        "            X_reg_transformed = preprocessor.transform(rated_games)\n",
        "            y_reg = rated_games['score_numeric']\n",
        "            reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            reg.fit(X_reg_transformed, y_reg)\n",
        "            joblib.dump(reg, reg_path)\n",
        "\n",
        "        if feature_names is not None:\n",
        "            skip_importances = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': clf.feature_importances_\n",
        "            }).sort_values(by='importance', ascending=False).head(5)\n",
        "            skip_importances['critic_id'] = critic_id\n",
        "            skip_importances['model_type'] = 'skip_prediction'\n",
        "            all_importances.append(skip_importances)\n",
        "\n",
        "            if reg:\n",
        "                score_importances = pd.DataFrame({\n",
        "                    'feature': feature_names,\n",
        "                    'importance': reg.feature_importances_\n",
        "                }).sort_values(by='importance', ascending=False).head(5)\n",
        "                score_importances['critic_id'] = critic_id\n",
        "                score_importances['model_type'] = 'score_prediction'\n",
        "                all_importances.append(score_importances)\n",
        "\n",
        "        # --- NEW: Find and store the index of the most representative tree ---\n",
        "        try:\n",
        "            forest_predictions = clf.predict(X_class_transformed)\n",
        "\n",
        "            best_tree_index = -1\n",
        "            max_accuracy = -1\n",
        "\n",
        "            for i, tree_estimator in enumerate(clf.estimators_):\n",
        "                tree_predictions = tree_estimator.predict(X_class_transformed)\n",
        "                accuracy = (forest_predictions == tree_predictions).mean()\n",
        "                if accuracy > max_accuracy:\n",
        "                    max_accuracy = accuracy\n",
        "                    best_tree_index = i\n",
        "\n",
        "            if best_tree_index != -1:\n",
        "                all_best_trees.append({\n",
        "                    'critic_id': critic_id,\n",
        "                    'best_tree_index': best_tree_index\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not determine best tree for critic {critic_id}. Error: {e}\")\n",
        "\n",
        "        X_all_games_transformed = preprocessor.transform(all_games_features)\n",
        "        skip_probs = clf.predict_proba(X_all_games_transformed)[:, 1]\n",
        "        pred_scores = reg.predict(X_all_games_transformed) if reg else np.nan\n",
        "\n",
        "        predictions_df = all_games_features[['id', 'name']].copy()\n",
        "        predictions_df['critic_id'] = critic_id\n",
        "        predictions_df['predicted_skip_probability'] = skip_probs\n",
        "        predictions_df['predicted_score'] = pred_scores\n",
        "        all_predictions.append(predictions_df)\n",
        "\n",
        "    if all_predictions:\n",
        "        final_pred_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        pred_output_path = os.path.join(results_dir, 'critic_predictions.csv')\n",
        "        final_pred_df.to_csv(pred_output_path, index=False)\n",
        "        print(f\"\\n✅ Predictions saved to '{pred_output_path}'\")\n",
        "    else:\n",
        "        print(\"\\nNo predictions were generated.\")\n",
        "\n",
        "    if all_importances:\n",
        "        final_imp_df = pd.concat(all_importances, ignore_index=True)\n",
        "        imp_output_path = os.path.join(results_dir, 'critic_feature_importances.csv')\n",
        "        final_imp_df.to_csv(imp_output_path, index=False)\n",
        "        print(f\"✅ Feature importances saved to '{imp_output_path}'\")\n",
        "\n",
        "    # --- NEW: Save Best Tree Indices to a CSV ---\n",
        "    if all_best_trees:\n",
        "        final_tree_df = pd.DataFrame(all_best_trees)\n",
        "        tree_output_path = os.path.join(results_dir, 'critic_best_trees.csv')\n",
        "        final_tree_df.to_csv(tree_output_path, index=False)\n",
        "        print(f\"✅ Best tree indices saved to '{tree_output_path}'\")"
      ],
      "metadata": {
        "id": "uoZ2gpTxGwxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_process(\n",
        "    data_dir=DATA_DIR,\n",
        "    models_dir=MODELS_DIR,\n",
        "    results_dir=RESULTS_DIR,\n",
        "    force_retrain=FORCE_RETRAIN\n",
        ")"
      ],
      "metadata": {
        "id": "YdMPMdxHGz9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the saved_models directory\n",
        "!zip -r /content/saved_models.zip /content/saved_models\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('/content/saved_models.zip')"
      ],
      "metadata": {
        "id": "P-_0kvrDG1KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPFYPIhIGUf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}