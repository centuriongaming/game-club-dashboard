{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf ./saved_models/\n",
        "!rm saved_models.zip"
      ],
      "metadata": {
        "id": "Snmqg_zQzrgp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"create_predictions.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Ocims907l3w9v02V_tbql8-hkNYfC5Fo\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "import ast\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Configuration for Local Environment ---\n",
        "DATA_DIR = 'data'\n",
        "MODELS_DIR = 'saved_models'\n",
        "RESULTS_DIR = 'results'\n",
        "\n",
        "# Set to True if you want to rebuild all models from scratch\n",
        "FORCE_RETRAIN = False\n",
        "\n",
        "# Minimum ratings required to build a model for a critic\n",
        "MIN_RATINGS_PER_CRITIC = 5\n",
        "\n",
        "import ast\n",
        "import json\n",
        "\n",
        "def parse_tags(data_str):\n",
        "    \"\"\"\n",
        "    Safely parses a string that could be a single or double-encoded list of tags.\n",
        "    Replaces spaces and hyphens in multi-word tags with underscores.\n",
        "    \"\"\"\n",
        "    if not isinstance(data_str, str):\n",
        "        return ''\n",
        "    try:\n",
        "        # First, try to evaluate the string as a Python literal.\n",
        "        parsed_data = ast.literal_eval(data_str)\n",
        "\n",
        "        # If the result of the first parse is *still* a string, it's likely\n",
        "        # double-encoded, so we parse it again using json.loads.\n",
        "        if isinstance(parsed_data, str):\n",
        "            final_list = json.loads(parsed_data)\n",
        "        else:\n",
        "            final_list = parsed_data\n",
        "\n",
        "        # Final check to ensure we have a list before joining.\n",
        "        if isinstance(final_list, list):\n",
        "            # Replace spaces and hyphens with underscores in each tag\n",
        "            processed_tags = [tag.replace(' ', '_').replace('-', '_') for tag in final_list]\n",
        "            return ' '.join(processed_tags)\n",
        "        else:\n",
        "            return ''\n",
        "    except (ValueError, SyntaxError, json.JSONDecodeError, TypeError):\n",
        "        # If any parsing step fails, return an empty string.\n",
        "        return ''\n",
        "\n",
        "def preprocess_data(games_df, ratings_df):\n",
        "    \"\"\"Merges, cleans, and prepares the data for modeling.\"\"\"\n",
        "    ratings_df = ratings_df.rename(columns={'id': 'rating_id'})\n",
        "\n",
        "    # --- CHANGED ---\n",
        "    # Merge on the new 'game_id' column, which should now exist in both files.\n",
        "    merged_df = pd.merge(ratings_df, games_df, on='game_id', how='left')\n",
        "\n",
        "    merged_df['will_skip'] = merged_df['score'].isnull() | (merged_df['score'] == 'skipped')\n",
        "    merged_df['score_numeric'] = pd.to_numeric(merged_df['score'], errors='coerce')\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        merged_df[col] = merged_df[col].fillna('')\n",
        "\n",
        "    if 'user_tags' in merged_df.columns:\n",
        "         merged_df['user_tags'] = merged_df['user_tags'].apply(parse_tags) # Use new parser\n",
        "\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "        if col in merged_df:\n",
        "             merged_df[col] = merged_df[col].str.replace(',', ' ')\n",
        "\n",
        "    merged_df['metacritic_score'] = pd.to_numeric(merged_df['metacritic_score'], errors='coerce')\n",
        "    merged_df['price_usd'] = pd.to_numeric(merged_df['price_usd'], errors='coerce')\n",
        "    merged_df['release_year'] = pd.to_datetime(merged_df['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def build_feature_pipeline():\n",
        "    \"\"\"Generates a scikit-learn pipeline to transform raw data into model-ready features.\"\"\"\n",
        "    numeric_features = ['metacritic_score', 'price_usd', 'release_year']\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    # Modify TfidfVectorizer for user_tags to include all n-grams\n",
        "    text_transformer_tags = TfidfVectorizer(stop_words='english', max_features=100, min_df=1) # ngram_range can be default now\n",
        "    text_transformer_genres = TfidfVectorizer(max_features=50, min_df=1)\n",
        "    text_transformer_devs = TfidfVectorizer(max_features=50, min_df=1)\n",
        "    text_transformer_pubs = TfidfVectorizer(max_features=50, min_df=1)\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('tags', text_transformer_tags, 'user_tags'),\n",
        "            ('genres', text_transformer_genres, 'developer_genres'),\n",
        "            ('devs', text_transformer_devs, 'developers'),\n",
        "            ('pubs', text_transformer_pubs, 'publishers')\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "# --- NEW FUNCTION ---\n",
        "def validate_schema(ratings_df, games_df):\n",
        "    \"\"\"\n",
        "    Validates that the dataframes have the new, required schema.\n",
        "    Halts execution if the schema is incorrect.\n",
        "    \"\"\"\n",
        "    print(\"Validating data schema...\")\n",
        "    ratings_cols = set(ratings_df.columns)\n",
        "    games_cols = set(games_df.columns)\n",
        "\n",
        "    errors = []\n",
        "\n",
        "    # Check 1: ratings.csv needs 'game_id'\n",
        "    if 'game_id' not in ratings_cols:\n",
        "        errors.append(\"ERROR: 'ratings.csv' is missing the required 'game_id' column.\")\n",
        "\n",
        "    # Check 2: games_details.csv needs 'game_id'\n",
        "    if 'game_id' not in games_cols:\n",
        "        errors.append(\"ERROR: 'games_details.csv' is missing the required 'game_id' column.\")\n",
        "\n",
        "    # Check 3: games_details.csv needs 'appid'\n",
        "    if 'appid' not in games_cols:\n",
        "        errors.append(\"ERROR: 'games_details.csv' is missing the required 'appid' column (which should be the renamed 'id' column).\")\n",
        "\n",
        "    # Check 4: games_details.csv should NOT have 'id'\n",
        "    if 'id' in games_cols:\n",
        "        errors.append(\"ERROR: 'games_details.csv' still contains an 'id' column. Please rename it to 'appid' and ensure the new 'game_id' column is present.\")\n",
        "\n",
        "    if errors:\n",
        "        print(\"\\n--- ðŸš¨ SCHEMA VALIDATION FAILED ðŸš¨ ---\")\n",
        "        for error in errors:\n",
        "            print(f\"- {error}\")\n",
        "        print(\"\\nPlease correct your CSV files based on the errors above and run the script again.\")\n",
        "        print(\"Reminder: 'games_details.csv' must now have 'game_id' (to match ratings.csv) and 'appid' (the old id).\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    print(\"âœ… Schema validation passed.\")\n",
        "    return True # Indicate success\n",
        "# --- END NEW FUNCTION ---\n",
        "\n",
        "\n",
        "def run_process(data_dir, models_dir, results_dir, force_retrain):\n",
        "    \"\"\"Main function to run the entire analysis pipeline.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    games_path = os.path.join(data_dir, 'games_details.csv')\n",
        "    ratings_path = os.path.join(data_dir, 'ratings.csv')\n",
        "    try:\n",
        "        games_df = pd.read_csv(games_path)\n",
        "        ratings_df = pd.read_csv(ratings_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Make sure your CSV files are in the '{data_dir}' directory.\")\n",
        "        return\n",
        "\n",
        "    # --- NEW: VALIDATION STEP ---\n",
        "    # Run the schema check. If it fails, stop the script.\n",
        "    if not validate_schema(ratings_df, games_df):\n",
        "        return\n",
        "\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data = preprocess_data(games_df.copy(), ratings_df.copy())\n",
        "\n",
        "    # --- CHANGED ---\n",
        "    # Prepare games_df for fitting the preprocessor manually\n",
        "    # Use 'game_id' as the unique identifier, not 'id'\n",
        "    preprocessed_games_df = games_df.drop_duplicates(subset=['game_id']).copy()\n",
        "\n",
        "    text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "    for col in text_cols:\n",
        "        preprocessed_games_df[col] = preprocessed_games_df[col].fillna('')\n",
        "    if 'user_tags' in preprocessed_games_df.columns:\n",
        "        preprocessed_games_df['user_tags'] = preprocessed_games_df['user_tags'].apply(parse_tags)\n",
        "    for col in ['developer_genres', 'developers', 'publishers']:\n",
        "        if col in preprocessed_games_df:\n",
        "             preprocessed_games_df[col] = preprocessed_games_df[col].str.replace(',', ' ')\n",
        "\n",
        "    preprocessed_games_df['metacritic_score'] = pd.to_numeric(preprocessed_games_df['metacritic_score'], errors='coerce')\n",
        "    preprocessed_games_df['price_usd'] = pd.to_numeric(preprocessed_games_df['price_usd'], errors='coerce')\n",
        "    preprocessed_games_df['release_year'] = pd.to_datetime(preprocessed_games_df['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "\n",
        "    print(\"Fitting the feature preprocessor on all games...\")\n",
        "    preprocessor = build_feature_pipeline()\n",
        "    preprocessor.fit(preprocessed_games_df)\n",
        "    joblib.dump(preprocessor, os.path.join(models_dir, 'preprocessor.joblib'))\n",
        "    print(\"Preprocessor fitted and saved.\")\n",
        "\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not get feature names from the preprocessor. Error: {e}\")\n",
        "        feature_names = None\n",
        "\n",
        "    critics = data['critic_id'].unique()\n",
        "    all_predictions = []\n",
        "    all_importances = []\n",
        "    all_best_trees = []\n",
        "\n",
        "    print(f\"\\nFound {len(critics)} critics. Starting model processing loop...\")\n",
        "    for critic_id in critics:\n",
        "        critic_data = data[data['critic_id'] == critic_id]\n",
        "\n",
        "        if len(critic_data) < MIN_RATINGS_PER_CRITIC:\n",
        "            print(f\"Skipping critic {critic_id}: not enough ratings.\")\n",
        "            continue\n",
        "\n",
        "        clf_path = os.path.join(models_dir, f'{critic_id}_classifier.joblib')\n",
        "        reg_path = os.path.join(models_dir, f'{critic_id}_regressor.joblib')\n",
        "\n",
        "        print(f\"--- Training models for critic: {critic_id} ---\")\n",
        "\n",
        "        X_class_transformed = preprocessor.transform(critic_data)\n",
        "        y_class = critic_data['will_skip']\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        clf.fit(X_class_transformed, y_class)\n",
        "        joblib.dump(clf, clf_path)\n",
        "\n",
        "        rated_games = critic_data.dropna(subset=['score_numeric'])\n",
        "        reg = None\n",
        "        if len(rated_games) >= MIN_RATINGS_PER_CRITIC / 2:\n",
        "            X_reg_transformed = preprocessor.transform(rated_games)\n",
        "            y_reg = rated_games['score_numeric']\n",
        "            reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            reg.fit(X_reg_transformed, y_reg)\n",
        "            joblib.dump(reg, reg_path)\n",
        "\n",
        "        if feature_names is not None:\n",
        "            skip_importances = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': clf.feature_importances_\n",
        "            }).sort_values(by='importance', ascending=False).head(5)\n",
        "            skip_importances['critic_id'] = critic_id\n",
        "            skip_importances['model_type'] = 'skip_prediction'\n",
        "            all_importances.append(skip_importances)\n",
        "\n",
        "            if reg:\n",
        "                score_importances = pd.DataFrame({\n",
        "                    'feature': feature_names,\n",
        "                    'importance': reg.feature_importances_\n",
        "                }).sort_values(by='importance', ascending=False).head(5)\n",
        "                score_importances['critic_id'] = critic_id\n",
        "                score_importances['model_type'] = 'score_prediction'\n",
        "                all_importances.append(score_importances)\n",
        "\n",
        "        # --- NEW: Find and store the index of the most representative tree ---\n",
        "        try:\n",
        "            forest_predictions = clf.predict(X_class_transformed)\n",
        "\n",
        "            best_tree_index = -1\n",
        "            max_accuracy = -1\n",
        "\n",
        "            for i, tree_estimator in enumerate(clf.estimators_):\n",
        "                tree_predictions = tree_estimator.predict(X_class_transformed)\n",
        "                accuracy = (forest_predictions == tree_predictions).mean()\n",
        "                if accuracy > max_accuracy:\n",
        "                    max_accuracy = accuracy\n",
        "                    best_tree_index = i\n",
        "\n",
        "            if best_tree_index != -1:\n",
        "                all_best_trees.append({\n",
        "                    'critic_id': critic_id,\n",
        "                    'best_tree_index': best_tree_index\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not determine best tree for critic {critic_id}. Error: {e}\")\n",
        "\n",
        "        # --- CHANGED ---\n",
        "        # Prepare games_df for prediction manually, using 'game_id' as the key\n",
        "        preprocessed_games_for_prediction = games_df.drop_duplicates(subset=['game_id']).copy()\n",
        "\n",
        "        text_cols = ['user_tags', 'developer_genres', 'developers', 'publishers']\n",
        "        for col in text_cols:\n",
        "            preprocessed_games_for_prediction[col] = preprocessed_games_for_prediction[col].fillna('')\n",
        "        if 'user_tags' in preprocessed_games_for_prediction.columns:\n",
        "            preprocessed_games_for_prediction['user_tags'] = preprocessed_games_for_prediction['user_tags'].apply(parse_tags)\n",
        "        for col in ['developer_genres', 'developers', 'publishers']:\n",
        "            if col in preprocessed_games_for_prediction:\n",
        "                 preprocessed_games_for_prediction[col] = preprocessed_games_for_prediction[col].str.replace(',', ' ')\n",
        "\n",
        "        preprocessed_games_for_prediction['metacritic_score'] = pd.to_numeric(preprocessed_games_for_prediction['metacritic_score'], errors='coerce')\n",
        "        preprocessed_games_for_prediction['price_usd'] = pd.to_numeric(preprocessed_games_for_prediction['price_usd'], errors='coerce')\n",
        "        preprocessed_games_for_prediction['release_year'] = pd.to_datetime(preprocessed_games_for_prediction['release_date'], errors='coerce', format='mixed').dt.year\n",
        "\n",
        "\n",
        "        X_all_games_transformed = preprocessor.transform(preprocessed_games_for_prediction)\n",
        "        skip_probs = clf.predict_proba(X_all_games_transformed)[:, 1]\n",
        "        pred_scores = reg.predict(X_all_games_transformed) if reg else np.nan\n",
        "\n",
        "        # --- CHANGED ---\n",
        "        # Get 'game_id' and 'name' for the output file\n",
        "        predictions_df = games_df.drop_duplicates(subset=['game_id'])[['game_id', 'name']].copy()\n",
        "        predictions_df['critic_id'] = critic_id\n",
        "        predictions_df['predicted_skip_probability'] = skip_probs\n",
        "        predictions_df['predicted_score'] = pred_scores\n",
        "        all_predictions.append(predictions_df)\n",
        "\n",
        "\n",
        "    if all_predictions:\n",
        "        final_pred_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        pred_output_path = os.path.join(results_dir, 'critic_predictions.csv')\n",
        "        final_pred_df.to_csv(pred_output_path, index=False)\n",
        "        print(f\"\\nâœ… Predictions saved to '{pred_output_path}'\")\n",
        "    else:\n",
        "        print(\"\\nNo predictions were generated.\")\n",
        "\n",
        "    if all_importances:\n",
        "        final_imp_df = pd.concat(all_importances, ignore_index=True)\n",
        "        imp_output_path = os.path.join(results_dir, 'critic_feature_importances.csv')\n",
        "        final_imp_df.to_csv(imp_output_path, index=False)\n",
        "        print(f\"âœ… Feature importances saved to '{imp_output_path}'\")\n",
        "\n",
        "    # --- NEW: Save Best Tree Indices to a CSV ---\n",
        "    if all_best_trees:\n",
        "        final_tree_df = pd.DataFrame(all_best_trees)\n",
        "        tree_output_path = os.path.join(results_dir, 'critic_best_trees.csv')\n",
        "        final_tree_df.to_csv(tree_output_path, index=False)\n",
        "        print(f\"âœ… Best tree indices saved to '{tree_output_path}'\")\n",
        "\n",
        "run_process(\n",
        "    data_dir=DATA_DIR,\n",
        "    models_dir=MODELS_DIR,\n",
        "    results_dir=RESULTS_DIR,\n",
        "    force_retrain=FORCE_RETRAIN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfLdzuI1zBx7",
        "outputId": "2c5bb12d-8a1e-480a-8c5a-83d69b25595d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Validating data schema...\n",
            "âœ… Schema validation passed.\n",
            "Preprocessing data...\n",
            "Fitting the feature preprocessor on all games...\n",
            "Preprocessor fitted and saved.\n",
            "\n",
            "Found 10 critics. Starting model processing loop...\n",
            "--- Training models for critic: 1 ---\n",
            "--- Training models for critic: 2 ---\n",
            "--- Training models for critic: 3 ---\n",
            "--- Training models for critic: 4 ---\n",
            "--- Training models for critic: 5 ---\n",
            "--- Training models for critic: 6 ---\n",
            "--- Training models for critic: 7 ---\n",
            "--- Training models for critic: 8 ---\n",
            "--- Training models for critic: 10 ---\n",
            "--- Training models for critic: 9 ---\n",
            "\n",
            "âœ… Predictions saved to 'results/critic_predictions.csv'\n",
            "âœ… Feature importances saved to 'results/critic_feature_importances.csv'\n",
            "âœ… Best tree indices saved to 'results/critic_best_trees.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzDY2exczCaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}